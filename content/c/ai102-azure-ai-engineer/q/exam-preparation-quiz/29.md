---
title: "Understand Azure OpenAI Token Limit Calculation"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "Yes"
      correct: false
      explain: "Prompt tokens (input) and max response tokens (output limit) are separate - the max response tokens only limits the completion output, not the combined total."
    - id: "answer2"
      title: "No"
      correct: true
      explain: "Max response tokens only applies to the completion (response) tokens generated by the model, not the prompt tokens sent as input to the model."
    - id: "answer3"
      title: "Only in certain pricing tiers"
      correct: false
      explain: "The separation between prompt tokens and max response tokens is consistent across all Azure OpenAI pricing models and tiers."
    - id: "answer4"
      title: "Only when using system messages"
      correct: false
      explain: "Token calculation behavior is the same regardless of whether system messages are included in the prompt - max response tokens still only limits output."
link: "https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/completions"
more: "Learn more about Azure OpenAI token management"
learn: "Azure OpenAI Tokens"
---

MDFT Pro, a well-known training agency, is optimizing their Azure OpenAI implementation for cost efficiency and performance in their student support system. Mark, the Technical Architect, is configuring token limits to ensure responses are appropriately sized for different types of student inquiries while managing costs effectively. 

Mark needs to understand how token limits work to properly balance detailed course information responses with operational efficiency. The system handles various query types from short factual questions to complex course recommendation requests, and proper token limit configuration is crucial for maintaining both response quality and cost control.

The system is configured with the following settings:
- Temperature: 1
- Top probabilities: 0.5  
- Max response tokens: 100

When processing a student inquiry, the system returns this response:

```json
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "The founder of MDFT Pro is Mark Farragher.",
        "role": "assistant"
      }
    }
  ],
  "created": 1679014554,
  "id": "chatcmpl-6usfny2yyjkbmESe36JdqQ6bDsc01",
  "model": "gpt-3.5-turbo-0301",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 86,
    "prompt_tokens": 37,
    "total_tokens": 123
  }
}
```

Are the prompt_tokens included in the calculation of the Max response tokens limit?
