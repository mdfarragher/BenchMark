---
title: "Define Multi-Dimensional Vector Representation In LLMs"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "An embedding"
      correct: true
      explain: "An embedding is a multi-dimensional vector representation that captures the semantic meaning of words or tokens, allowing the language model to understand relationships between different terms."
    - id: "answer2"
      title: "Attention"
      correct: false
      explain: "Attention is a mechanism that helps models focus on relevant parts of the input sequence when processing information, not the vector representation of individual words."
    - id: "answer3"
      title: "A completion"
      correct: false
      explain: "A completion refers to the text output generated by a language model in response to a prompt, not the internal vector representation of tokens."
    - id: "answer4"
      title: "A transformer"
      correct: false
      explain: "A transformer is the overall neural network architecture used in modern language models, not the vector representation assigned to individual words or tokens."
link: "https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/understand-embeddings"
more: "Learn more about embeddings"
learn: "Text Embeddings"
---

Claire, the Machine Learning Engineer at MDFT Pro, is developing an intelligent search system for the company's extensive digital course library. MDFT Pro, a well-known training agency, has thousands of educational documents, course materials, and learning resources that need to be made searchable based on semantic meaning rather than just keyword matching. Students should be able to find relevant content even when using different terminology than what appears in the course materials. Claire needs to understand how large language models represent text internally to implement effective semantic search capabilities that can match student queries with conceptually similar educational content.

What term describes the multi-dimensional vector representation that captures the semantic meaning of words in large language models?