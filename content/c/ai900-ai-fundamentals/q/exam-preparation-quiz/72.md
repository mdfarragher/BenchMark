---
title: "Calculate True Positives From Confusion Matrix"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "5"
      correct: false
      explain: "This represents false positives (predicted positive but actually negative), not correctly predicted positives."
    - id: "answer2"
      title: "11"
      correct: true
      explain: "This represents true positives - cases that were correctly predicted as positive (predicted 1 and actual 1)."
    - id: "answer3"
      title: "1033"
      correct: false
      explain: "This represents false negatives (predicted negative but actually positive), not correctly predicted positives."
    - id: "answer4"
      title: "13951"
      correct: false
      explain: "This represents true negatives (correctly predicted as negative), not correctly predicted positives."
link: "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml"
more: "Learn more about confusion matrices"
learn: "Confusion Matrix Analysis"
---

Claire, the Model Validation Analyst at MDFT Pro, is evaluating a classification model that predicts whether students will successfully complete their certification programs. MDFT Pro, a well-known training agency, has developed this model to identify students who may need additional support early in their learning journey. Claire is analyzing the model's performance using a confusion matrix from test data to understand how accurately the model identifies students who will successfully complete their programs versus those who may struggle or withdraw.

Based on the following confusion matrix for the student success prediction model, how many correctly predicted positives (true positives) are there?

| | Actual Success | Actual Non-Success |
|---------------:|:--------------:|:------------------:|
| **Predicted Success** | 11 | 5 |
| **Predicted Non-Success** | 1033 | 13951 |

How many correctly predicted positive cases are shown in this confusion matrix?

