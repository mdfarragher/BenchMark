---
title: "Calculate False Negatives From Confusion Matrix"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "5"
      correct: false
      explain: "This represents false positives (predicted positive but actually negative), not false negatives."
    - id: "answer2"
      title: "11"
      correct: false
      explain: "This represents true positives (correctly predicted as positive), not false negatives."
    - id: "answer3"
      title: "1033"
      correct: true
      explain: "This represents false negatives - cases that were actually positive but incorrectly predicted as negative (predicted 0 but actual 1)."
    - id: "answer4"
      title: "13951"
      correct: false
      explain: "This represents true negatives (correctly predicted as negative), not false negatives."
link: "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml"
more: "Learn more about confusion matrices"
learn: "Confusion Matrix Analysis"
---

Mark, the Student Risk Assessment Analyst at MDFT Pro, is evaluating the performance of a machine learning model that identifies students at risk of not completing their certification programs. MDFT Pro, a well-known training agency, uses this model to trigger early intervention programs and additional support services. Mark is particularly concerned about false negatives because these represent at-risk students who were incorrectly classified as successful, meaning they won't receive the support they actually need to complete their programs successfully.

Based on the following confusion matrix for the student risk prediction model, how many false negative cases are there?

| | Actual At-Risk | Actual Not At-Risk |
|---------------:|:--------------:|:------------------:|
| **Predicted At-Risk** | 11 | 5 |
| **Predicted Not At-Risk** | 1033 | 13951 |

How many false negative predictions does this confusion matrix show?

