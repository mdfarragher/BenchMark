---
title: "Evaluate Dataset Bias From Confusion Matrix"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "Yes"
      correct: true
      explain: "The dataset shows extreme class imbalance with only 1044 positive cases versus 13956 negative cases, indicating significant bias toward the negative class."
    - id: "answer2"
      title: "No"
      correct: false
      explain: "The dataset demonstrates clear imbalance with roughly 93% negative cases and only 7% positive cases, which represents substantial bias that can affect model performance."
link: "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml"
more: "Learn more about class imbalance"
learn: "Class Imbalance Analysis"
---

Claire, the Model Fairness Auditor at MDFT Pro, is reviewing machine learning models used for student success prediction to ensure they provide equitable outcomes across different student populations. MDFT Pro, a well-known training agency, is committed to supporting all students effectively and wants to identify potential biases in their predictive models that could lead to unfair treatment or missed opportunities for student support. Claire is analyzing the confusion matrix from a model trained on historical student data to assess whether the training dataset reflects balanced representation or shows problematic patterns that could affect model reliability.

Based on the following confusion matrix showing actual versus predicted student success outcomes, does this dataset exhibit bias?

| | Actual Success | Actual Struggle |
|---------------:|:--------------:|:---------------:|
| **Predicted Success** | 11 | 5 |
| **Predicted Struggle** | 1033 | 13951 |

Does this confusion matrix indicate bias in the underlying dataset?

