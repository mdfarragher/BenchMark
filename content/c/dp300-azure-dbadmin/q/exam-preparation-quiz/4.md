---
title: "Load JSON Files Into Spark Pool Tables"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "Load the data by using PySpark"
      correct: true
      explain: "PySpark automatically infers and maintains source data types when loading JSON files, making it the best solution for this requirement."
    - id: "answer2"
      title: "Load the data by using the OPENROWSET T-SQL command in an Azure Synapse Analytics serverless SQL pool"
      correct: false
      explain: "While OPENROWSET can read JSON files, it may not preserve all data types as accurately as PySpark, especially with varying structures."
    - id: "answer3"
      title: "Use a Get Metadata activity in Azure Data Factory"
      correct: false
      explain: "Get Metadata activity retrieves metadata about data but doesn't load data into tables or maintain data types during the loading process."
    - id: "answer4"
      title: "Use a Conditional Split transformation in an Azure Synapse data flow"
      correct: false
      explain: "Conditional Split is used for routing data based on conditions, not for loading data or maintaining source data types."
link: "https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-overview"
more: "Learn more about Apache Spark In Azure Synapse Analytics"
learn: "Apache Spark"
---
You work for MDFT Pro, a well-known training agency that uses Azure Synapse Analytics for data analytics. Mark, the Lead Data Engineer, has assigned you to work with an Apache Spark pool named **StudentAnalytics**. 

The company receives course completion data and assessment results from various learning platforms in JSON format, stored in an Azure Data Lake Storage Gen2 container. The challenge is that each platform uses slightly different JSON structures and data types for similar fields. Mark needs you to load these files into tables in the Spark pool while ensuring that the original data types from each source system are preserved for accurate analysis.

What should you do?
