---
title: "Evaluate Statistical Measures in Fabric"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "Yes"
      explain: "The explain().show() method is used to display the execution plan of a DataFrame operation, showing how Spark will process the data. It does not calculate any statistical measures like min, max, mean, or standard deviation. To calculate these statistics, you should use the describe() function instead, which automatically computes count, mean, standard deviation, min, and max values for all numeric columns in the DataFrame."
    - id: "answer2"
      title: "No"
      correct: true

link: "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html"
more: "Learn more about the describe function"
learn: "DataFrame Describe"
---

You work for MDFT Pro, a well-known training agency that uses Microsoft Fabric for data analysis. You have a Fabric tenant that contains business data in OneLake. 

You use a Fabric notebook to read the data into a Spark DataFrame. You need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the numeric columns.

Solution: You use the following PySpark expression:

```python
df.explain().show()
```

Does this meet the goal?
