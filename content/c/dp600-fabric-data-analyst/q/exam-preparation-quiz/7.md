You have a Fabric workspace that uses the default Spark starter pool and runtime version 1,2.

You plan to read a CSV file named Sales.raw.csv in a lakehouse, select columns, and save the data as a Delta table to the managed area of the lakehouse. Sales_raw.csv contains 12 columns.

You have the following code.

from pyspark.sqi.functions import year
(spark
.read
.format ("csv")
.option("header", true)
.load("Files/sales_raw.csv")
.select( "SalesOrderNunber", "OrderDates", "CustomerName", "UnitPrice")
.withColumn("year", year("OrderOate"))
.write
.partitionBy("year")
.saveAsTable("sales")

Which statements are true?

- The Spark engine will read only the SalesOrderNumber, OrderDate, CustomerName and UnitPrice columns from Sales_raw.csv. (correct)
- Removing the partition will reduce the execution time of the query.
- Adding inferSchema="true" to the options will increase the execution time of the query. (correct)
