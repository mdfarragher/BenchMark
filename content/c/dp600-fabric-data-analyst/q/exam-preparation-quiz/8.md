---
title: "Choose the Best Tool for Large-Scale Data Processing"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "A Microsoft Power B1 report that uses core visuals"
      explain: "Power BI reports are designed for data visualization and analysis, but they are not suitable for processing large datasets with one billion items. Power BI reports are better for creating interactive dashboards and visualizations after the data has been processed."
    - id: "answer2"
      title: "The PySpark library in a Fabric notebook"
      correct: true
    - id: "answer3"
      title: "The pandas library in a Fabric notebook"
      explain: "While pandas is a powerful data manipulation library, it is not designed for processing one billion items efficiently. Pandas operates in-memory and would struggle with such a large dataset. It's better suited for smaller datasets that can fit in memory."
    - id: "answer4"
      title: "T-SQL queries in a Fabric SQL endpoint"
      explain: "While T-SQL is powerful for data manipulation, it's not the best choice for processing JSON files with one billion items. T-SQL would require loading the JSON data into a relational format first, which would be inefficient and time-consuming for such a large dataset."

link: "https://spark.apache.org/docs/latest/api/python/index.html"
more: "Learn more about PySpark"
learn: "PySpark"
---

You work for MDFT Pro, a training agency that operates a Fabric tenant containing JSON files in OneLake. These files contain one billion training records, including student enrollments, course completions, and assessment results.

As a data analyst at MDFT Pro, you need to perform time series analysis of these training records to identify trends in student performance and course popularity.

You need to transform the data, visualize the data to find insights, perform anomaly detection, and share the insights with other business users. The solution must meet the following requirements:

- Use parallel processing.
- Minimize the duplication of data.
- Minimize how long it takes to load the data.

What should you use to transform and visualize the data?
