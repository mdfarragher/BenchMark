---
title: "Ingest Large Files with Event Triggering"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "Data pipeline"
      correct: true
      explain: "A data pipeline with event-based triggers is the correct choice for ingesting large files when they are added. Data pipelines support Azure Blob Storage and OneLake events that can trigger execution when files are created, provide high throughput for large file ingestion through optimized copy activities, and can ingest data without transformations by directly copying files to the lakehouse."
    - id: "answer2"
      title: "Environment"
      correct: false
      explain: "Environments in Fabric are used to manage libraries and runtime settings for Spark workloads, not for data ingestion. They provide configuration for notebooks and Spark jobs but don't have built-in capabilities for event-triggered file ingestion."
    - id: "answer3"
      title: "KQL queryset"
      correct: false
      explain: "KQL querysets are used for querying data in KQL databases and eventhouses, not for ingesting data into lakehouses. They are designed for analytical queries on streaming data, not for batch file ingestion workflows."
    - id: "answer4"
      title: "Dataflow Gen2"
      correct: false
      explain: "While Dataflow Gen2 can ingest data into lakehouses, it doesn't provide native event-triggered execution when files are added. Dataflows are better suited for scheduled or manual transformations with a low-code interface, not for high-throughput event-driven ingestion of large files without transformations."
link: "https://learn.microsoft.com/en-us/fabric/real-time-hub/tutorial-build-event-driven-data-pipelines"
more: "Learn more about event-driven pipelines"
learn: "Event-Driven Data Pipelines"
---
You work for MDFT Pro, a well-known training agency that receives daily video recordings of training sessions from multiple campuses. Claire, a Data Engineer at MDFT Pro, manages a Fabric lakehouse called TrainingVideos that stores these recordings for later transcription and analysis. Each campus uploads one large video file (approximately 500 GB) to an Azure Data Lake Storage account at the end of each business day. These files must be ingested into the lakehouse without any transformations to preserve the original quality and metadata. The ingestion process should automatically trigger whenever a new file appears in the storage account, ensuring minimal delay between file upload and availability in the lakehouse. Additionally, the solution must provide optimal throughput to handle the large file sizes efficiently, minimizing the time required to complete each ingestion operation.

Which type of Fabric item should Claire use to ingest the data?
