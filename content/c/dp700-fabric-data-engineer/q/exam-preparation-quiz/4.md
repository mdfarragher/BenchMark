---
title: "Ingest Large Files with Event-Based Triggering"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "Eventstream"
      correct: true
      explain: "Eventstream provides real-time ingestion with high throughput and can be configured to trigger when new files are added. It supports streaming large files into a lakehouse without transformations and offers optimized performance for continuous data ingestion patterns."
    - id: "answer2"
      title: "Dataflow Gen2"
      correct: false
      explain: "Dataflow Gen2 is designed for data transformation scenarios using a low-code interface. While it can ingest data, it's not optimized for high-throughput ingestion of large files without transformations, and it doesn't provide native event-based triggering capabilities."
    - id: "answer3"
      title: "Streaming dataset"
      correct: false
      explain: "Streaming datasets are primarily used for real-time dashboards and reports in Power BI. They are not designed for ingesting large batch files into a lakehouse and don't provide the required throughput for 500 GB files."
    - id: "answer4"
      title: "Data pipeline"
      correct: false
      explain: "While data pipelines support event-based triggers and can handle large files, they typically provide lower throughput compared to eventstreams for continuous ingestion scenarios. Data pipelines are better suited for orchestrating complex ETL workflows rather than simple high-throughput file ingestion."
link: "https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-streaming-data"
more: "Learn more about streaming data into lakehouses"
learn: "Streaming Data Into Lakehouses"
---
You work for MDFT Pro, a well-known training agency that offers online certification courses worldwide. Claire, a Data Engineer at MDFT Pro, manages the company's student activity data platform. Each day, the company's learning management system generates a large data file (approximately 500 GB) containing detailed student engagement metrics, course completion records, and assessment results. These files are stored in an external Azure Data Lake Storage account and must be ingested into a Fabric lakehouse called StudentData for further analysis. The ingestion process should not apply any transformations to preserve the raw data, and it must automatically trigger whenever a new file appears in the storage account. Additionally, the solution needs to provide the highest possible throughput to minimize the time between file arrival and data availability for analysis.

Which type of Fabric item should Claire use to ingest the data?
