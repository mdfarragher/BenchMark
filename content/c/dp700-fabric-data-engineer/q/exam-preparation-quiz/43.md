---
title: "Copy Data from On-Premises Database to Warehouse"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "An Apache Spark job definition"
      correct: false
      explain: "Apache Spark job definitions are used to run Spark applications and notebooks in Fabric, but they are not designed for copying data from on-premises SQL Server databases. Spark jobs would require writing custom code and don't provide the built-in connectivity through an on-premises data gateway that's needed for this scenario."
    - id: "answer2"
      title: "A data pipeline"
      correct: true
      explain: "A data pipeline with a Copy data activity is the correct solution for copying data from an on-premises SQL Server database to a Fabric warehouse. Data pipelines support on-premises data gateway connections (version 3000.214.2 or higher) and provide the Copy data assistant to easily configure the source connection, select tables/views, and specify the warehouse destination. Note that external staging storage (Azure Blob Storage or ADLS Gen2) is required when copying to a warehouse."
    - id: "answer3"
      title: "A Dataflow Gen1 dataflow"
      correct: false
      explain: "Dataflow Gen1 is being deprecated in Fabric and is not recommended for new implementations. While it might support on-premises connections, it's not the appropriate tool for loading data into a Fabric warehouse. Additionally, Dataflow Gen1 lacks the full capabilities and performance optimizations available in data pipelines."
    - id: "answer4"
      title: "An eventstream"
      correct: false
      explain: "Eventstreams are designed for real-time streaming data scenarios, capturing events from sources like Azure Event Hubs, IoT devices, or Kafka. They are not suitable for copying batch data from on-premises SQL Server databases through a data gateway. Eventstreams don't support on-premises data gateway connections."
link: "https://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data-pipelines"
more: "Learn more about ingesting data into warehouse using pipelines"
learn: "Ingest Data into Warehouse Using Pipelines"
---
You work for MDFT Pro, a well-known training agency that maintains student enrollment and course completion records in an on-premises database. Mark, a Data Platform Engineer at MDFT Pro, manages a Fabric workspace that contains a warehouse named StudentRecordsWarehouse. The company's primary student management system runs on an on-premises Microsoft SQL Server database named EnrollmentDB that contains tables for student registrations, course schedules, instructor assignments, and assessment results. This database is accessed through an on-premises data gateway that has been installed and configured in the Fabric workspace. Mark needs to copy data from multiple tables in EnrollmentDB to StudentRecordsWarehouse on a regular schedule to enable analytics and reporting on student enrollment trends, course popularity, and completion rates. The solution should provide a reliable, supported method for moving data from the on-premises environment to the cloud-based warehouse.

Which item should Mark use to copy data from EnrollmentDB to StudentRecordsWarehouse?
