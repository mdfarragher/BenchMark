---
title: "Orchestrate Notebook Execution and Model Refresh"
type: "question"
layout: "multiple"
answers:
    - id: "answer1"
      title: "Add Notebook1 to an Apache Spark job definition"
      correct: false
      explain: "While you can create Apache Spark job definitions to run notebooks, this approach doesn't provide the orchestration needed to refresh Model1 after Notebook1 completes successfully. Spark job definitions execute independently and cannot chain semantic model refresh activities."
    - id: "answer2"
      title: "Add Pipeline1 to an Apache Spark job definition"
      correct: false
      explain: "Apache Spark job definitions are designed to run Spark applications and notebooks, not data pipelines. You cannot add a data pipeline to a Spark job definition. Pipelines and Spark job definitions are separate Fabric item types with different purposes."
    - id: "answer3"
      title: "Add an Execute Notebook1 activity to Pipeline1"
      correct: true
      explain: "Adding Execute Notebook1 activity to Pipeline1 allows you to orchestrate the notebook execution within a pipeline. You can then schedule Pipeline1 to run every weekday at 8:00 AM, and chain additional activities (like refreshing Model1) to execute after the notebook completes successfully using OnSuccess dependencies."
    - id: "answer4"
      title: "Add an Execute Notebook2 activity to Pipeline1"
      correct: false
      explain: "Adding Notebook2 to Pipeline1 would run it on a schedule, but the requirement is to execute Notebook2 when a file is saved to Azure Blob Storage. This is an event-driven scenario that requires Real-Time Hub event configuration, not scheduled pipeline execution."
    - id: "answer5"
      title: "Add a Refresh Model1 activity to Pipeline1"
      correct: true
      explain: "The Semantic model refresh activity allows you to refresh Power BI semantic models from a pipeline. By adding this activity to Pipeline1 with an OnSuccess dependency on the Execute Notebook1 activity, Model1 will automatically refresh after Notebook1 completes successfully. This works for both Import and Direct Lake mode semantic models."
    - id: "answer6"
      title: "From Real-Time hub, configure the execution of Notebook2"
      correct: true
      explain: "Real-Time Hub supports event-driven automation for Fabric items. You can subscribe to Azure Blob Storage events (such as blob created or updated) and configure an action to run Notebook2 when files are saved to the storage container. This enables near real-time execution based on storage events rather than schedules."
    - id: "answer7"
      title: "From Real-Time hub, configure the execution of Pipeline1"
      correct: false
      explain: "Configuring Pipeline1 execution from Real-Time Hub would make the pipeline run in response to events rather than on the required weekday schedule. The requirement is for Notebook1 (via Pipeline1) to run every weekday at 8:00 AM, which is a scheduled execution, not an event-driven one."
    - id: "answer8"
      title: "For Model1, enable the Keep your Direct Lake data up to date option"
      correct: false
      explain: "While enabling this option would automatically update Model1 when lakehouse data changes, the requirement specifies that Model1 must refresh when Notebook1 has executed successfully, not when data changes. This creates a specific dependency that requires orchestration through a pipeline refresh activity rather than automatic updates."
link: "https://learn.microsoft.com/en-us/fabric/data-factory/semantic-model-refresh-activity"
more: "Learn more about semantic model refresh activity in pipelines"
learn: "Semantic Model Refresh Activity"
---
You work for MDFT Pro, a well-known training agency that manages student analytics and reporting. Claire, a Data Orchestration Specialist at MDFT Pro, works with a Fabric workspace named AnalyticsWorkspace that contains several items as shown in the table below:

| Name                  | Type           |
| --------------------- | -------------- |
| StudentDataProcessor  | Notebook       |
| CourseFileProcessor   | Notebook       |
| StudentLakehouse      | Lakehouse      |
| ETLOrchestrator       | Data pipeline  |
| StudentMetricsModel   | Semantic model |

StudentMetricsModel is a Direct Lake semantic model with the "Keep your Direct Lake data up to date" option currently disabled. Claire needs to configure the execution of these items to meet the following requirements:

- StudentDataProcessor must execute every weekday at 8:00 AM to process overnight student activity data
- CourseFileProcessor must execute when instructors save new course material files to an Azure Blob Storage container
- StudentMetricsModel must refresh after StudentDataProcessor has executed successfully to ensure reports show the latest processed data

How should Claire orchestrate this solution? Select all that apply.
