---
title: "Configure Copy Data Activity from Snowflake to Warehouse"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "Degree of copy parallelism"
      correct: false
      explain: "Degree of copy parallelism controls how many concurrent connections are used to copy data in parallel, which can improve performance for large data transfers. While this setting can optimize copy speed, it's not required for copying from Snowflake to a Fabric warehouse. The essential configuration for this scenario is staging."
    - id: "answer2"
      title: "Fault tolerance"
      correct: false
      explain: "Fault tolerance settings allow the copy activity to skip incompatible rows during data transfer, such as rows that don't match the destination schema. While useful for handling data quality issues, fault tolerance is optional and not a requirement for copying data from Snowflake to a Fabric warehouse."
    - id: "answer3"
      title: "Enable staging"
      correct: true
      explain: "Enable staging is required when copying data from Snowflake to a Fabric warehouse. Staging uses an interim Azure Blob storage instance to enable the built-in staged copy process. When the destination data store or format isn't natively compatible with the Snowflake COPY command, staged copy provides better throughput by exporting data from Snowflake into staging storage, copying it to the destination, and then cleaning up temporary data. You need to configure an External Azure Blob Storage connection in the Settings tab for staging."
    - id: "answer4"
      title: "Enable logging"
      correct: false
      explain: "Enable logging captures detailed activity execution logs, including session logs, error files, and data consistency verification results. While logging is helpful for troubleshooting and auditing, it's optional and not required for the basic functionality of copying data from Snowflake to a Fabric warehouse."
link: "https://learn.microsoft.com/en-us/fabric/data-factory/connector-snowflake-copy-activity"
more: "Learn more about configuring Snowflake in a copy activity"
learn: "Configure Snowflake Copy Activity"
---
You work for MDFT Pro, a well-known training agency that maintains course catalog data in Snowflake. Claire, a Data Integration Specialist at MDFT Pro, is developing a data pipeline named CourseDataPipeline to migrate historical course information from a Snowflake data warehouse into a Fabric warehouse named MDFTCourseCatalog. The Snowflake source contains several years of course descriptions, learning objectives, prerequisite requirements, instructor assignments, and student enrollment patterns. Claire needs to add a Copy data activity to CourseDataPipeline that will efficiently transfer this data from the Snowflake data source to MDFTCourseCatalog while ensuring compatibility with Snowflake's COPY command and optimal transfer performance. The Copy data activity must be properly configured to handle the data movement between these two platforms.

What should Claire configure in the Copy data activity settings?
