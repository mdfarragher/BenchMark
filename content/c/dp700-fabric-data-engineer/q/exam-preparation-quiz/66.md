---
title: "Choose Storage For Multi-Engine Access"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "A lakehouse"
      correct: true
      explain: "A lakehouse supports reading data through multiple engines including T-SQL (via SQL analytics endpoint), KQL (via shortcuts or queries), and Apache Spark, while also supporting Spark-based writes. It's designed specifically for semi-structured data and multi-engine access patterns."
    - id: "answer2"
      title: "An eventhouse"
      correct: false
      explain: "An eventhouse is optimized for real-time analytics and KQL queries on streaming data, but it doesn't provide native T-SQL access or support Spark-based writes as the primary write mechanism."
    - id: "answer3"
      title: "A datamart"
      correct: false
      explain: "A datamart is designed for structured relational data and business intelligence workloads, but it doesn't support Apache Spark for writing data or KQL for querying, making it unsuitable for this requirement."
    - id: "answer4"
      title: "A warehouse"
      correct: false
      explain: "While a warehouse supports T-SQL queries, it doesn't support KQL queries or Apache Spark as the write mechanism. Warehouses are optimized for structured data and SQL-based operations."
link: "https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-overview"
more: "Learn more about Fabric lakehouses"
learn: "Fabric Lakehouse Overview"
---
You work as a Data Platform Architect for Mark at MDFT Pro, a well-known training agency. MDFT Pro operates a Fabric workspace that handles various types of student and course data. Mark has assigned you the task of storing semi-structured data from student activity logs, course feedback forms, and learning management system events. The data architecture team at MDFT Pro needs to be able to read this data using multiple query engines: T-SQL for reporting dashboards, KQL for real-time analytics, and Apache Spark for machine learning workloads. However, all write operations will be performed exclusively using Spark to maintain consistency in the data ingestion pipeline. You need to choose the appropriate storage solution that supports this multi-engine access pattern.

What should you use to store the data?
