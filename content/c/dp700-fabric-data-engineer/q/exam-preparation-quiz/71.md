---
title: "Share Spark Session Between Notebooks"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "Enable high concurrency for notebooks"
      correct: true
      explain: "High concurrency mode allows multiple notebooks to share the same Spark session, enabling collaboration and resource sharing between Notebook1 and Notebook2. This is the specific feature designed for sharing Spark sessions across notebooks."
    - id: "answer2"
      title: "Enable dynamic allocation for the Spark pool"
      correct: false
      explain: "Dynamic allocation automatically adjusts the number of executors based on workload, but it doesn't enable sharing of Spark sessions between notebooks. Each notebook would still have its own separate session."
    - id: "answer3"
      title: "Change the runtime version"
      correct: false
      explain: "Changing the runtime version affects which Spark libraries and features are available but does not enable session sharing between notebooks. Both notebooks would still run in separate Spark sessions."
    - id: "answer4"
      title: "Increase the number of executors"
      correct: false
      explain: "Increasing executors provides more parallel processing capacity but doesn't allow notebooks to share the same Spark session. Each notebook would still have its own isolated session with its allocated executors."
link: "https://learn.microsoft.com/en-us/fabric/data-engineering/configure-high-concurrency-session-notebooks"
more: "Learn more about high concurrency mode in Fabric notebooks"
learn: "Notebook High Concurrency Mode"
---
You work as a Data Engineering Coordinator for Mark at MDFT Pro, a well-known training agency. MDFT Pro operates a Fabric workspace named Workspace1 that contains a notebook named Notebook1, which processes student enrollment data and course analytics using Apache Spark. Mark has asked you to create a new notebook named Notebook2 in the same workspace to perform additional analysis on the same data. To improve efficiency and enable collaboration between the two notebooks, you need to ensure that Notebook2 can attach to the same Apache Spark session as Notebook1, allowing them to share data and computational resources without duplicating the Spark context setup.

What should you do?
