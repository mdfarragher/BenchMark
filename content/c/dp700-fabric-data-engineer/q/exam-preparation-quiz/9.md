---
title: "Build a Dynamic Data Ingestion Pipeline"
type: "question"
layout: "multiple"
answers:
    - id: "answer1"
      title: "Add a Lookup activity to query ControlObject"
      correct: true
      explain: "The Lookup activity is used to retrieve data from the control table containing the list of schemas and tables to be copied. This is the appropriate activity for querying database tables."
    - id: "answer2"
      title: "Add a Get Metadata activity to query ControlObject"
      correct: false
      explain: "Get metadata activities are used to retrieve metadata properties like file size and modification date, not to query database tables. For retrieving data from a control table, a Lookup activity is the appropriate choice."
    - id: "answer3"
      title: "Add a ForEach activity to iterate over the list of tables from the Lookup result"
      correct: true
      explain: "The ForEach activity is used to iterate over each table returned by the Lookup activity. This allows the pipeline to process each table individually in a single execution."
    - id: "answer4"
      title: "Add an Until activity to iterate over the list of tables"
      correct: false
      explain: "An Until activity is designed for conditional looping (repeating until a condition is met), not for iterating over a collection of items. A ForEach activity is the proper choice for iterating over the list of tables."
    - id: "answer5"
      title: "Add a Copy data activity inside the ForEach activity"
      correct: true
      explain: "The Copy data activity inside the ForEach loops processes each table individually, copying data from the source Azure SQL database to the Fabric lakehouse for each table in the control object."
    - id: "answer6"
      title: "Add a Copy data activity inside an Until activity"
      correct: false
      explain: "While Copy data activities can be used to copy data, they should be nested inside a ForEach activity (not Until) to iterate over the tables from the control object."
    - id: "answer7"
      title: "Add a Script activity to manually define each table to copy"
      correct: false
      explain: "Script activities execute code but are not needed for this metadata-driven approach. The combination of Lookup, ForEach, and Copy data activities handles the dynamic ingestion without manual table definitions."
    - id: "answer8"
      title: "Add a Filter activity to filter the table list from ControlObject"
      correct: false
      explain: "While filtering could be useful in some scenarios, it's not a required step for building the basic dynamic ingestion pipeline. The Lookup, ForEach, and Copy data activities form the core solution."
link: "https://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data-pipelines"
more: "Learn more about ingesting data with pipelines"
learn: "Ingest Data with Pipelines"
---
You work for MDFT Pro, a well-known training agency that offers certification courses. Mark, the Data Engineer, is building a data integration solution to consolidate student and course data from an Azure SQL database into a Fabric lakehouse. 

The source database contains 25 tables including **Students**, **Courses**, **Enrollments**, **Assessments**, and other related tables. Rather than creating separate pipelines for each table, Mark wants to implement a metadata-driven approach using a control table. 

He creates a table called **ControlObject** in a Fabric warehouse that lists all the schemas and table names to be copied. This control table will make it easy to add or remove tables from the ingestion process without modifying the pipeline code. Mark needs to design a single data pipeline that can dynamically read the control table and copy all listed tables in one execution.

Which three actions should Mark perform in sequence to build this dynamic ingestion pipeline?
