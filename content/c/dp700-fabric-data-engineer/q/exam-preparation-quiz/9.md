---
title: "Build a Dynamic Data Ingestion Pipeline"
type: "question"
layout: "single"
answers:
    - id: "answer1"
      title: "1. Add a Lookup activity to query Control.Object; 2. Add a ForEach activity to iterate over tables; 3. Add a Copy data activity inside ForEach"
      correct: true
      explain: "This is the correct sequence for implementing metadata-driven dynamic ingestion. First, the Lookup activity queries the control table to retrieve the list of schemas and tables. Then, the ForEach activity iterates over each table in the list. Finally, the Copy data activity inside the ForEach processes each table individually, copying data from the source to the lakehouse."
    - id: "answer2"
      title: "1. Add a Get metadata activity to query Control.Object; 2. Add a ForEach activity to iterate over tables; 3. Add a Copy data activity inside ForEach"
      correct: false
      explain: "Get metadata activities are used to retrieve metadata properties like file size and modification date, not to query database tables. For retrieving data from a control table, a Lookup activity is the appropriate choice."
    - id: "answer3"
      title: "1. Add a Lookup activity to query Control.Object; 2. Add an Until activity to iterate over tables; 3. Add a Copy data activity inside Until"
      correct: false
      explain: "While the Lookup activity is correct, an Until activity is designed for conditional looping (repeating until a condition is met), not for iterating over a collection of items. A ForEach activity is the proper choice for iterating over the list of tables."
    - id: "answer4"
      title: "1. Add a Get metadata activity to query Control.Object; 2. Add an Until activity to iterate over tables; 3. Add a Copy data activity inside Until"
      correct: false
      explain: "This sequence uses both incorrect activities. Get metadata doesn't query tables, and Until doesn't iterate over collections. The correct combination is Lookup for querying and ForEach for iteration."
link: "https://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data-pipelines"
more: "Learn more about ingesting data with pipelines"
learn: "Ingest Data with Pipelines"
---
You work for MDFT Pro, a well-known training agency that offers certification courses. Mark, a Data Engineer at MDFT Pro, is building a data integration solution to consolidate student and course data from an Azure SQL database into a Fabric lakehouse. The source database contains 25 tables including Students, Courses, Enrollments, Assessments, and other related tables. Rather than creating separate pipelines for each table, Mark wants to implement a metadata-driven approach using a control table. He creates a table called Control.Object in a Fabric warehouse that lists all the schemas and table names to be copied. This control table will make it easy to add or remove tables from the ingestion process without modifying the pipeline code. Mark needs to design a single data pipeline that can dynamically read the control table and copy all listed tables in one execution.

Which three actions should Mark perform in sequence to build this dynamic ingestion pipeline?
