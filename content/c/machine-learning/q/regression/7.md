---
title: "How can you fix this problem?"
type: "question"
layout: "single"
answers:
    - id: answer1
      title: "Increase the 'number of epochs' hyperparameter"
      explain: "Number of epochs specifies how long the model is trained. Increasing the training time will not solve the problem."

    - id: answer2
      title: "Decrease the 'number of epochs' hyperparameter"
      explain: "Number of epochs specifies how long the model is trained. Descreasing the training time will not solve the problem."

    - id: answer3
      title: "Increase the 'step size' hyperparameter"
      explain: "Increasing the step size will not fix the problem. Gradient descent is wildly jumping around on the loss surface because the step size is too large."
      
    - id: answer4
      title: "Decrease the 'step size' hyperparameter"
      correct: true
---

You are building a regression model to predict the wind speed at any point on earth during any time of the day. You are training your model with Gradient Descent and notice that the loss value does not decrease smoothly during training. Instead, it changes chaotically and the training often terminates with a low-qualty model. 

How can you fix this problem?