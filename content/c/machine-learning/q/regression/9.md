---
title: "How can you fix this problem?"
type: "question"
layout: "single"
answers:
    - id: answer1
      title: "Increase the 'number of epochs' hyperparameter"
      explain: "Number of epochs specifies how long the model is trained. Increasing the training time will not solve the problem."

    - id: answer2
      title: "Decrease the 'number of epochs' hyperparameter"
      explain: "Number of epochs specifies how long the model is trained. Descreasing the training time will not solve the problem."

    - id: answer3
      title: "Increase the 'step size' hyperparameter"
      correct: true
      
    - id: answer4
      title: "Decrease the 'step size' hyperparameter"
      explain: "Decreasing the step size will make the problem worse, not better. Gradient descent is getting stuck in false minima on the loss surface because the step size is too small."

more: "There are many reasons why you might see this behavior, but in this scenario we assume that you're dealing with a very irregular loss surface with lots of false minima. With a step size that is set too small, Gradient descent keeps getting stuck in a false minima and seldom finds the true optimal solution, so you're seeing different loss values after every training run."
---

You are building a regression model to predict the wind speed at any point on earth during any time of the day. You are training your model with Gradient Descent and you notice that every time you run a training, you get different final loss values. Some models are great, some average, and some very poor. 

How can you fix this problem?